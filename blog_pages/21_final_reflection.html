<h2>Final Reflection</h2>

<h3>Overview</h3>

<p>
    During the course of my blog project, I gained a significant amount of experience in both identifying and
    contextualizing computational problems for a non-technical audience. Throughout my computer science degree, I had
    focused heavily on educating myself about the different kinds of risks posed by digital technologies in order to
    prepare myself to go into industry as a software developer—and to hopefully avoid causing similar human rights
    violations to the issues brought up in my blog, my Wakelets, my Kumu maps, and our class discussions. While I am no
    longer shocked by the types of rights violations I discussed in by blog, I learned a lot from the experience of
    applying techniques from our discussions on framing human rights issues on these topics.
</p>

<h3>Successes</h3>

<p>
    My initial goal for the project was to apply the following four techniques from Price’s discussion about framing
    human rights issues:
</p>

<ol>
    <li>Generating issues by disseminating information about specific features of algorithms and the human rights
        affected by them</li>
    <li>Establishing networks for generating broad support for change in legal data protections and corporate
        responsibility in the United States</li>
    <li>Grafting a new norm onto existing norms, specifically in relation to the unnecessary complexity in describing
        how specific algorithms work</li>
    <li>Using a transnational Socratic method to demand that companies justify their use of specific algorithms and data
        collection techniques</li>
</ol>

<p>
    With regard to disseminating information about data collection and data processing, my blog successfully outlined
    the
    different actors in the data collection process. I decided to focus on two main aspects of identifying the main
    actors
    by educating readers on the scope of big data, issues surrounding data brokers, and the actors and rights holders
    created by the General Data Protection Regulation (GDPR). Given the format of the blog, I intentionally organized
    these
    posts such that a reader could understand each post in isolation but that the flow of topics builds upon previous
    posts.
    Had there been more time, I would have liked to go deeper into the personal effects of data brokers and the GDPR. I
    included interactive examples (such as the linked data broker websites) since they quickly and clearly convey the
    almost
    dystopian nature of big data processing, but I was only able to scratch the surface of the issue. My blog has
    sections
    that briefly address corporate responsibility and networks for addressing rights violations, but those aspects are
    mainly addressed by the embedded Kumu maps—especially the main map about the overall connections between the fields
    of
    computer science, statistics, human rights, and law.
</p>

<p>
    One of the most successful aspects of my blog was in shifting the perception of big data and creating new norms
    among my
    classmates. Feedback from my classmates made it clear that my contributions to the blog, comments during class, and
    discussion posts helped to shift their perception of artificial intelligence and machine learning and that they feel
    more comfortable in their understanding of the major concerns to human rights from data processing.
</p>

<h3>Shifting Focus Throughout the Project</h3>

<p>
    Throughout the project, my main goals were maintained, but my overall approach drastically changed. I initially
    planned to organize my blog posts around the following weeks:
</p>

<ol>
    <li>the rights of the data subject/obligations of data controllers/processors,
    </li>
    <li>
        sources of data and data brokers (focusing on personally identifiable information and data derived from the
        Internet of Things [ex. smart phones, smart devices, connected devices]),
    </li>
    <li>case study of natural language processing applications (ex. chat bots and large language models),
    </li>
    <li>case study of general artificial intelligence and automated decision making (ex. credit scoring algorithms and
        the data they rely on),
    </li>
    <li>case study of computer vision applications (ex. facial recognition),
    </li>
    <li>possibly some form of reflection/suggestions/what can be done (individually/collectively and by which actors,
        likely will focus on policy suggestions)
    </li>
</ol>

<p>
    However, as I began writing blog posts, I quickly realized that two posts each were not enough to outline the actors
    described by the GDPR and their relation to data brokers. In the few weeks before we began writing our blog posts, I
    had done a significant amount of research about case studies, data laws, and current research about data processing
    that I had intended to support blog posts about case studies, but their inclusion would have gone far beyond the
    scope of the blog. Instead, I aggregated and organized some of my most significant findings—articles, laws, outside
    blog posts, reports, research papers, etc.—in Wakelets at the end of my blog series.
</p>

<p>
    I initially planned to tackle the issue of mass data collection/distribution and data processing through blog post
    on different fields of computer science or specific actors, including artificial intelligence (AI), machine learning
    (ML), and natural language processing (NLP), and had intended to demonstrate how some of these tools worked and make
    observations about their effect. However, as I began outlining the topics and talking about my topic with some of my
    classmates, two things became abundantly clear. First, amongst my classmates, there was some confusion about the
    scope of big data and the main actors, so I shifted my focus toward clarifying this issue and educating readers on
    the recent development of data rights—especially in relation to the existence of data brokers. Second, my intention
    of writing programs to demonstrate some of the negative effects of machine learning and large language models was
    going to limit my ability to dive deeply into the smaller set of topics that I chose. I had begun testing out some
    of the large language models to demonstrate some of their effects in blog posts, but I soon realized that I was not
    going to be able to effectively program them to generate simple but clearly interpretable examples. I decided that
    educating readers about the risks of data processing was of a greater priority in order for my blog to have a
    stronger impact and better address my initial goals.
</p>

<p>
    Unfortunately, due to the sheer amount of time I spent aggregating content and designing interactive Kumu maps,
    there was not enough time to cover all of the topics I had hoped—especially case studies of rights violations and
    online surveillance. However, in my final Kumu map, I spent time aggregating articles about as many of the examples
    as possible that I brought up in class throughout the semester. The articles I chose do a really good job of
    outlining the main concerns of each case and, even without a technical background, the terrifying nature of these
    cases is extremely clear.
</p>

<p>
    I was especially disappointed not to have time to go into more detail about a few specific aspects of data
    processing that I had researched for my Wakelets and Kumu maps. I had hoped to cover some of the issues of deepfakes
    and revenge pornography. Deepfake technologies have gained a significant amount of attention for their use in
    misinformation (especially political misinformation); however, they pose the greatest risk to women as a new form of
    unregulated revenge pornography. Current estimates put pornographic deepfakes at <a
        href="https://www.forbes.com/sites/chenxiwang/2019/11/01/deepfakes-revenge-porn-and-the-impact-on-women/?sh=76d874441f53">approximately
        96% of all deepfake
        videos generated and the vast majority of videos are of women</a>. These videos pose a novel risk to women's
    safety as a
    direct result of big data—specifically due to the significant advancements in deepfake technologies being easier to
    use. At first, deepfakes required a significant number of images of the intended subject, but improvements have
    significantly reduced the number of necessary images to generate hyper-realistic videos. Similarly, I had intended
    to cover some of the issues with Twitter bots and misinformation. Specifically, I had intended to analyze the scope
    of the issue and educate readers on both the widespread nature of the problem and the difficulty in identifying
    synthetic users. If time had permitted, I had also intended to touch upon the issue of content moderation and
    predictive policing algorithms. Fortunately, I was able to contribute to the topic of content moderation through
    class discussions and our content moderation assignment; however, there were less opportunities to address the issue
    of predictive policing. My senior thesis dealt with the issue of obscuring discrimination with the choice of
    quantifiable fairness metric in validation studies for the COMPAS recidivism prediction algorithm, but there are a
    variety of other forms of <a
        href="https://www.brennancenter.org/our-work/research-reports/predictive-policing-explained">predictive
        policing</a> to which I would have liked to expose my classmates. There are
    numerous other applications of police algorithms such as criminal risk prediction algorithms, crime hot spot
    predictions, surveillance technologies, and many more that pose significant risks to human rights of privacy,
    non-discrimination, equal treatment under the law, and more.
</p>

<h3>Kumu Maps</h3>

<p>
    I tend to organize my sources and ideas for researching projects/assignments in JSON data files, so I intentionally
    organized them to allow my research to be translated into nodes of a Kumu map. Toward the beginning of the class, I
    spent a significant amount of time researching the limitations of Kumu and came across information about <a
        href="https://docs.kumu.io/guides/blueprints">Kumu Blueprints</a> while reading through all of their
    documentation. Unfortunately, Kumu has extremely limited documentation for any features beyond the basic
    user-interface features, so I spent time testing the limitations of programming the underlying Kumu map to import.
</p>

<p>
    The Kumu Blueprint files can be found in my <a
        href="https://github.com/sedona-thomas/Big-Data-and-Data-Processing-Blog">Github Repository</a> for the
    blog series under the “Kumu Map” folder. A significant amount of my work included tagging nodes, adding links, and
    labeling node types which can either be seen by clicking the nodes in the online interface or by finding the nodes
    in the Kumu Blueprint.
</p>

<p>
    In the end, I created three main Kumu maps:
</p>

<p>
    The main Kumu map outlines as much as possible about the connections between different fields of study, existing
    laws and regulations, and examples of technologies for each field (and intersecting fields).
</p>
<iframe src="https://embed.kumu.io/4ff1031a5ed3d449ecc18044f8e653db" width="940" height="600" frameborder="0"></iframe>

<p>
    The artificial intelligence/machine learning Kumu map describes a variety of tools for different fields and links
    relevant information about each tool.
</p>
<iframe src="https://embed.kumu.io/0aeea49fadfc19fb40d917695704a91a" width="940" height="600" frameborder="0"></iframe>

<p>
    The data broker Kumu map outlines the different clients of data brokers (including specific companies that aggregate
    personal data) and the types of personal information that they collect.
</p>
<iframe src="https://embed.kumu.io/9b6120db2b6539cc63f11052ce1b226a" width="940" height="600" frameborder="0"></iframe>


<h3>Wakelet Pages</h3>
<p>
    Similar to my organization process allowing for translation of my notes into Kumu maps, I organized the nodes such
    that they could be used to create Wakelet pages for some of the topics I was unable to cover throughout the blog. I
    ended up with five Wakelet pages:
</p>

<ul>
    <li><a href="https://wakelet.com/wake/vn_g7fzCLUEzeyiFNa1k">Content Moderation</a></li>
    <li><a href="https://wakelet.com/wake/tg95ej8U4lxSYEtdLWYZO">AI/ML Tools</a></li>
    <li><a href="https://wakelet.com/wake/1n71gT8Tu8bVlqljrU4NL">AI Ethics Codes and Regulations</a></li>
    <li><a href="https://wakelet.com/wake/gIkt7gaKNoEdSU2Dfbf8g">Data Rights Violations (including case studies)</a>
    </li>
    <li><a href="https://wakelet.com/wake/A4DkEWpBWNndrRiAEVE07">Disability and Technology</a></li>
</ul>

<p>
    My personal notes include a significant amount more of information for each of the links I have saved and can be
    found
    in my <a href="https://github.com/sedona-thomas/Big-Data-and-Data-Processing-Blog">Github Repository</a> for
    the blog series under “Wakelet/links.json”. Specifically, a significant number of links
    have
    manually created tags and information about when articles or technologies were released/made publicly available.
</p>