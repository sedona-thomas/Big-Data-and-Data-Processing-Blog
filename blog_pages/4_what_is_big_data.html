<h2>Big Data</h2>
<div id="big_data">
    <p>
        <a
            href="https://www.oracle.com/big-data/what-is-big-data/#:~:text=is%20Big%20Data%3F-,Big%20data%20defined,especially%20from%20new%20data%20sources.">
            <i>Big data</i>
        </a>
        describes large, complex datasets that allow for more detailed analysis than
        traditional datasets. It often comes from new data sources, such as social media,
        and can be used to reveal highly specific patterns in the dataset that previous
        technologies were unable to identify within a reasonable timeframe. It is typically 
        unstructured and requires a significant amount of processing to create useful insights, 
        and it is described by its volume, velocity, and variety. Volume describes the quantity 
        of data. The data volume refers to both in number of records and the number of features 
        known about each data entry and can be understood as both the number of rows and columns 
        in a spreadsheet. Velocity describes the speed at which data is generated, aquired, and 
        processed. This can refer to both the collection and processing steps of big data analysis. 
        Variety describes the different types of data that are included in the dataset. Data can be 
        structured (such as a traditional spreadsheet, labeled image, or transcribed audio) or 
        unstructured (such as a raw text file, image, or video). Each of these data types can be used 
        in a variety of applications, and the drastic rise in utility of big data has been strongly 
        motivated by expontential improvements in computing power over the past couple of decades.
    </p>

    <p>

        For example, in the 1970s, a single terabyte
        of computer storage would have costed almost a 185 million dollars, but now it costs less than
        15 dollars. Computing speed has also experienced dramatic improvements. Since the early 2000s,
        the worldâ€™s fastest supercomputers have become almost 100,000 times faster. These advancements
        were initially speculated by Gordon E. Moore in 1965 when he identified that the number of
        transistors (one of the basic building blocks of computational electronics) doubled every two
        years. This phenomenon is now known as
        <a href="https://www.amazon.com/Invitation-Computer-Science-G-Michael-Schneider/dp/1337561916">
            "Moore's Law"</a>
        and has continued to be true for the past 50 years. This form of rapid advancement has been
        unparalleled compared to other fields. For context, the Intel 404 processor had 2,300 transistors
        in 1971, and the nVidiaGeForce 6800 Ultra had 3 billion transistors in 2011.
    </p>
    
    <p>
        These improvements in both storage and computing power have increased access to the processing of
        extremely large datasets and has creating a large market for buying and selling vast amounts of
        personal data. This data is then processed for a variety of reasons by entities called 
        <a href="https://epic.org/issues/consumer-privacy/data-brokers/">"data brokers"</a>.
    </p>
</div>